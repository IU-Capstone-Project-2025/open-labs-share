{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DB_PATH = 'faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaccef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"data/predator-pray-22/pdfs\"\n",
    "pdf_docs = []\n",
    "for file in os.listdir(pdf_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_dir, file))\n",
    "        pdf_docs.extend(loader.load())\n",
    "\n",
    "code_dir = \"data/predator-pray-22/code\"\n",
    "code_docs = []\n",
    "for file in os.listdir(code_dir):\n",
    "    if file.endswith(\".java\"):\n",
    "        loader = TextLoader(os.path.join(code_dir, file), encoding=\"utf-8\")\n",
    "        code_docs.extend(loader.load())\n",
    "\n",
    "all_docs = pdf_docs + code_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d2772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "split_docs = splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embedding_model)\n",
    "db.save_local(RAG_DB_PATH)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3384591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda\n",
      "/tmp/ipykernel_33531/203845955.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_gen)\n"
     ]
    }
   ],
   "source": [
    "qwen_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85f88e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prompt_message import PromptMessage\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "\"\"\"\n",
    "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
    "You are a helpful assistant that explains programming assignments.\n",
    "Your task is to explain key terms, notions and user's questions. \n",
    "Do not give any hints or direct solution of task even if you asked.\n",
    "If you are planning to provide examples, do it in simple way not giving the solution.\n",
    "Answer user's question in plain English and suggest how to approach it.\n",
    "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
    "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
    "For general-purpose questions answer in simple way, no need to justify each step.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def format_prompt(user_message: str,  chat_history: tp.List[BaseMessage], context: str = None) -> str:\n",
    "    '''\n",
    "    Formats prompt for llm\n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    for message in chat_history[:-1]:\n",
    "        if message.type == \"human\":\n",
    "            role = \"user\"\n",
    "        elif message.type == \"ai\":\n",
    "            role = \"assistant\"\n",
    "        elif message.type == \"system\":\n",
    "            role = \"system\"\n",
    "\n",
    "        history.append(PromptMessage(\n",
    "            role=role,\n",
    "            content=message.content\n",
    "        ))\n",
    "\n",
    "    if context:\n",
    "        history.append(PromptMessage(\n",
    "            role=\"system\",\n",
    "            content=context\n",
    "        ))\n",
    "\n",
    "    history.append(PromptMessage(\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    ))\n",
    "\n",
    "    for h in history:\n",
    "        print(h)\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def format_model_response(response: str):\n",
    "    matches = list(re.finditer(r\"<\\|im_start\\|>assistant\", response))\n",
    "    if not matches:\n",
    "        return response.strip()\n",
    "    last = matches[-1].start()\n",
    "\n",
    "    return response[last + len(\"<|im_start|>assistant\"):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d787fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = db.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"{doc.page_content}\\n\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"retrieve\" in last_message.content.lower():\n",
    "        result = retrieve(last_message.content)\n",
    "        prompt = format_prompt(\n",
    "            user_message=last_message.content,\n",
    "            chat_history=state[\"messages\"],\n",
    "            context=result\n",
    "        )\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "    else:\n",
    "        prompt = format_prompt(\n",
    "            user_message=last_message.content,\n",
    "            chat_history=state[\"messages\"],\n",
    "        )\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": format_model_response(response)\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56cb561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "866b473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='system' content=\"\\nBelow is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\\nYou are a helpful assistant that explains programming assignments.\\nYour task is to explain key terms, notions and user's questions. \\nDo not give any hints or direct solution of task even if you asked.\\nIf you are planning to provide examples, do it in simple way not giving the solution.\\nAnswer user's question in plain English and suggest how to approach it.\\nYou are enhanced AI model with previous prompt storage. Provide answers considering history\\nDo not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\\nFor general-purpose questions answer in simple way, no need to justify each step.\\n\"\n",
      "role='user' content='Hi my name is Alex'\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi my name is Alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Alex! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "config={\"configurable\":{\"thread_id\":1}}\n",
    "input_message = \"Hi my name is Alex\"\n",
    "\n",
    "\n",
    "input_state={\n",
    "    \"messages\":[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": input_message}\n",
    "    ]\n",
    "}\n",
    "response_state=graph.invoke(input_state,config=config)\n",
    "for message in response_state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ee60edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role='system' content=\"\\nBelow is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\\nYou are a helpful assistant that explains programming assignments.\\nYour task is to explain key terms, notions and user's questions. \\nDo not give any hints or direct solution of task even if you asked.\\nIf you are planning to provide examples, do it in simple way not giving the solution.\\nAnswer user's question in plain English and suggest how to approach it.\\nYou are enhanced AI model with previous prompt storage. Provide answers considering history\\nDo not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\\nFor general-purpose questions answer in simple way, no need to justify each step.\\n\"\n",
      "role='user' content='Hi my name is Alex'\n",
      "role='assistant' content='Hello, Alex! How can I assist you today?'\n",
      "role='user' content='What I have asked you before'\n",
      "role='assistant' content=\"I apologize, but I don't remember your previous questions. Can you please provide more details or rephrase your question so I can help you better?\"\n",
      "role='user' content='Did I say hello'\n",
      "role='assistant' content='Yes, you did say \"hello\" to me. Is there something specific you would like to know or discuss?'\n",
      "role='user' content='Who you are'\n",
      "role='assistant' content='I am an AI language model designed to assist users with various tasks and queries. How can I help you today?'\n",
      "role='user' content='What can you do'\n",
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi my name is Alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Alex! How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What I have asked you before\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I apologize, but I don't remember your previous questions. Can you please provide more details or rephrase your question so I can help you better?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Did I say hello\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Yes, you did say \"hello\" to me. Is there something specific you would like to know or discuss?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who you are\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am an AI language model designed to assist users with various tasks and queries. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What can you do\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can answer questions, provide explanations, and offer assistance on a wide range of topics. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "input_message = \"What can you do\"\n",
    "input_state={\n",
    "    \"messages\":[\n",
    "        {\"role\": \"user\", \"content\": input_message}\n",
    "    ]\n",
    "}\n",
    "response_state=graph.invoke(input_state,config=config)\n",
    "for message in response_state[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
