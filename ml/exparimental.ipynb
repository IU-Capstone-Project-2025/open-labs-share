{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate,\\\n",
    "    ChatPromptTemplate,\\\n",
    "    SystemMessagePromptTemplate,\\\n",
    "    HumanMessagePromptTemplate,\\\n",
    "    MessagesPlaceholder\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DB_PATH = 'faiss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaccef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"data/predator-pray-22/pdfs\"\n",
    "pdf_docs = []\n",
    "for file in os.listdir(pdf_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_dir, file))\n",
    "        pdf_docs.extend(loader.load())\n",
    "\n",
    "code_dir = \"data/predator-pray-22/code\"\n",
    "code_docs = []\n",
    "for file in os.listdir(code_dir):\n",
    "    if file.endswith(\".java\"):\n",
    "        loader = TextLoader(os.path.join(code_dir, file), encoding=\"utf-8\")\n",
    "        code_docs.extend(loader.load())\n",
    "\n",
    "all_docs = pdf_docs + code_docs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d2772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "split_docs = splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n",
    "db = FAISS.from_documents(split_docs, embedding_model)\n",
    "db.save_local(RAG_DB_PATH)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3384591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda\n",
      "/tmp/ipykernel_5939/203845955.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_gen)\n"
     ]
    }
   ],
   "source": [
    "qwen_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f88e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that explains programming assignments.\n",
    "Your task is to explain key terms, notions and user's questions. \n",
    "Do not give any hints or direct solution of task even if you asked.\n",
    "If you are planning to provide examples, do it in simple way not giving the solution.\n",
    "Answer user's question in plain English and suggest how to approach it.\n",
    "\"\"\"\n",
    "\n",
    "def format_prompt(user_message: str, context: str = None) -> str:\n",
    "    return (\n",
    "        f\"System: {SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"Context: {context if context is not None else None}\"\n",
    "        f\"User: {user_message}\\n\\n\"\n",
    "        f\"Assistant:\"\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d787fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = db.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"{doc.page_content}\\n\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n",
    "\n",
    "def query_or_respond(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"retrieve\" in last_message.content.lower():\n",
    "        result = retrieve(last_message.content)\n",
    "        prompt = format_prompt(last_message.content, result)\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "    else:\n",
    "        prompt = format_prompt(last_message.content)\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "tools = ToolNode([retrieve])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "graph_builder.add_node(query_or_respond)\n",
    "graph_builder.add_node(tools)\n",
    "\n",
    "graph_builder.set_entry_point(\"query_or_respond\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"query_or_respond\",\n",
    "    tools_condition,\n",
    "    {END: END, \"tools\": \"tools\"},\n",
    ")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd51e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "retrieve what is submittion format \n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "System: \n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "\n",
      "\n",
      "Context: 5 Submission\n",
      "The submission consists of two parts: your code and a report documenting your submission. The\n",
      "code and report must be submitted before the deadline, by both members of your pair .\n",
      "5.1 Code\n",
      "You have to submit a Jar of your project to the “Assignment 3: Code Submission” link in the\n",
      "Assignment 3 section on the PPA KEATS page, before the due date. The Jar ﬁle must contain\n",
      "your source code, i.e., the *.java ﬁles, and runs on BlueJ .\n",
      "5.2 Report\n",
      "\n",
      "\n",
      "private final String STEP_PREFIX = \"Step: \";\n",
      "    private final String POPULATION_PREFIX = \"Population: \";\n",
      "    private JLabel stepLabel, population, infoLabel;\n",
      "    private FieldView fieldView;\n",
      "    \n",
      "    // A map for storing colors for participants in the simulation\n",
      "    private Map<Class, Color> colors;\n",
      "    // A statistics object computing and storing simulation information\n",
      "    private FieldStats stats;\n",
      "User: retrieve what is submittion format \n",
      "\n",
      "Assistant: The submission format for Assignment 3 requires two main components:\n",
      "1. **Code**: You need to submit a Java JAR file containing all your source code files (.java).\n",
      "2. **Report**: A document that documents your submission, explaining the steps taken, results obtained, and any additional notes.\n",
      "\n",
      "To approach this submission format:\n",
      "- **Code Submission**:\n",
      "  - Go to the \"Assignment 3: Code Submission\" link on the PPA KEATS page.\n",
      "  - Upload your JAR file as specified in the instructions.\n",
      "  - Make sure your JAR file includes all necessary.java files and can run on BlueJ.\n",
      "\n",
      "- **Report Submission**:\n",
      "  - Write a detailed report that explains the following:\n",
      "    - The steps you followed to implement the simulation.\n",
      "    - The results of running the simulation (e.g., number of steps, population size, etc.).\n",
      "    - Any observations or insights you gained from the simulation.\n",
      "    - Any challenges you faced and how you overcame them.\n",
      "\n",
      "By following these steps, you will ensure that both your code and report are submitted correctly before the deadline. If you encounter any issues, review the guidelines carefully and check the documentation provided on the PPA KEATS page. Happy coding! 🚀\n"
     ]
    }
   ],
   "source": [
    "input_message = \"retrieve what is submittion format \"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
