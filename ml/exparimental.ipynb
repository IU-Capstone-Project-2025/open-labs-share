{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a0979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/miniconda3/envs/ml/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "from langchain_core.documents.base import Document\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DB_PATH = 'faiss'\n",
    "SCORE_THRESHOLD = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "339df927",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c1f1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_docs(path: str, idx: str) -> tp.List[Document]:\n",
    "    pdf_docs = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(path, file))\n",
    "            loaded_docs = loader.load()\n",
    "            for doc in loaded_docs:\n",
    "                doc.metadata[\"assignment_id\"] = idx\n",
    "\n",
    "            pdf_docs.extend(loaded_docs)\n",
    "\n",
    "    return pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4fe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_EXTENSIONS = [\n",
    "    \".py\", \".java\", \".js\", \".ts\", \".cpp\", \".c\", \".h\", \".hpp\", \".cs\", \".go\", \".rb\", \".php\", \".swift\",\n",
    "    \".kt\", \".scala\", \".rs\", \".m\", \".sh\", \".bat\", \".pl\", \".lua\", \".dart\", \".html\", \".css\", \".json\", \".xml\",\n",
    "    \".yaml\", \".yml\", \".sql\", \".dockerfile\", \"Dockerfile\", \".env\", \".ini\", \".cfg\", \".conf\", \".toml\",\n",
    "    \".md\", \".rst\", \".ipynb\", \".ps1\", \".vb\", \".asp\", \".jsp\", \".tsx\", \".jsx\", \".groovy\", \".gradle\",\n",
    "    \".make\", \"Makefile\", \".cmake\", \".tex\"\n",
    "]\n",
    "\n",
    "def is_code_file(filename: str) -> bool:\n",
    "    return any(filename.endswith(ext) for ext in CODE_EXTENSIONS)\n",
    "\n",
    "def load_code_docs(path: str, idx: str) -> tp.List[Document]:\n",
    "    code_docs = []\n",
    "\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if is_code_file(file):\n",
    "                path = os.path.join(root, file)\n",
    "                loader = TextLoader(path, encoding=\"utf-8\")\n",
    "                loaded = loader.load()\n",
    "                for doc in loaded:\n",
    "                    doc.metadata[\"assignment_id\"] = idx\n",
    "                code_docs.extend(loaded)\n",
    "\n",
    "    return code_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22abe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"data/predator-pray-22/pdfs\"\n",
    "code_dir = \"data/predator-pray-22/code\"\n",
    "\n",
    "all_docs = load_pdf_docs(pdf_dir, \"1\") + load_code_docs(code_dir, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d2772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    separators = [\n",
    "        \"\\n/**\",      # Javadoc start\n",
    "        \"\\n/*\",       # Block comment\n",
    "        \"\\n//\",       # Line comment\n",
    "        \"\\nclass \",   # Java class declaration\n",
    "        \"\\ninterface \",  # Java interface declaration\n",
    "        \"\\npublic \",  # public method/field\n",
    "        \"\\nprivate \", # private method/field\n",
    "        \"\\nprotected \", # protected method/field\n",
    "        \"\\nstatic \",  # static method or field\n",
    "        \"\\nvoid \",    # method with no return\n",
    "        \"\\nint \",     # common return type\n",
    "        \"\\nString \",  # String declarations\n",
    "        \"\\n\",         # fallback: line break\n",
    "        \" \"           # fallback: space\n",
    "    ]\n",
    ")\n",
    "split_docs = splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "#     model_kwargs={\"device\": device},\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n",
    "# db = FAISS.from_documents(split_docs, embedding_model)\n",
    "# db.save_local(RAG_DB_PATH)\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     k=3,\n",
    "#     search_kwargs={\n",
    "#         \"score_threshold\": SCORE_THRESHOLD,\n",
    "#         \"filter\": {\"assignment_id\": \"1\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96606db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "db = FAISS.load_local(RAG_DB_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    k=3,\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": SCORE_THRESHOLD,\n",
    "        \"filter\": {\"assignment_id\": \"1\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3384591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "qwen_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f88e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.schemas import PromptMessage\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "\"\"\"\n",
    "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
    "You are a helpful assistant that explains programming assignments.\n",
    "Your task is to explain key terms, notions and user's questions. \n",
    "Do not give any hints or direct solution of task even if you asked.\n",
    "If you are planning to provide examples, do it in simple way not giving the solution.\n",
    "Answer user's question in plain English and suggest how to approach it.\n",
    "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
    "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
    "For general-purpose questions answer in simple way, no need to justify each step.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def format_prompt(user_message: str,  chat_history: tp.List[BaseMessage], context: str = None) -> str:\n",
    "    '''\n",
    "    Formats prompt for llm\n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    for message in chat_history[:-1]:\n",
    "        if message.type == \"human\":\n",
    "            role = \"user\"\n",
    "        elif message.type == \"ai\":\n",
    "            role = \"assistant\"\n",
    "        elif message.type == \"system\":\n",
    "            role = \"system\"\n",
    "\n",
    "        history.append(PromptMessage(\n",
    "            role=role,\n",
    "            content=message.content\n",
    "        ))\n",
    "\n",
    "    if context:\n",
    "        history.append(PromptMessage(\n",
    "            role=\"system\",\n",
    "            content=context\n",
    "        ))\n",
    "\n",
    "    history.append(PromptMessage(\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    ))\n",
    "\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def format_model_response(response: str):\n",
    "    matches = list(re.finditer(r\"<\\|im_start\\|>assistant\", response))\n",
    "    if not matches:\n",
    "        return response.strip()\n",
    "    last = matches[-1].start()\n",
    "\n",
    "    return response[last + len(\"<|im_start|>assistant\"):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d787fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from agent.schemas import RAGState\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def retrieve(state: RAGState) -> str:\n",
    "    \"\"\"Retrieve relevant (< threshold) information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(state.query)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"{doc.page_content}\\n\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return {\"docs\": serialized}\n",
    "\n",
    "\n",
    "def route_rag_usage(state: RAGState) -> str:\n",
    "    return \"query_rag_llm\" if state.docs else \"query_llm\"\n",
    "\n",
    "\n",
    "def query_rag_llm(state: RAGState) -> dict:\n",
    "    messages = state.msg_state[\"messages\"]\n",
    "    \n",
    "    prompt = format_prompt(\n",
    "        user_message=state.query,\n",
    "        chat_history=state.msg_state[\"messages\"],\n",
    "        context=state.docs\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    new_messages = messages + [\n",
    "        HumanMessage(content=state.query),\n",
    "        AIMessage(content=format_model_response(response))\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"msg_state\": MessagesState(\n",
    "            thread_id=state.msg_state[\"thread_id\"],\n",
    "            messages=new_messages\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def query_llm(state: RAGState) -> dict:\n",
    "    messages = state.msg_state[\"messages\"]\n",
    "\n",
    "\n",
    "    prompt = format_prompt(\n",
    "        user_message=state.query,\n",
    "        chat_history=state.msg_state[\"messages\"],\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    new_messages = messages + [\n",
    "        HumanMessage(content=state.query),\n",
    "        AIMessage(content=format_model_response(response))\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"msg_state\": MessagesState(\n",
    "            thread_id=state.msg_state[\"thread_id\"],\n",
    "            messages=new_messages\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f1b9",
   "metadata": {},
   "source": [
    "**Run docker**\n",
    "\n",
    "``\n",
    "sudo docker run --name chat-postgres --env-file ml/.env -v pgdata:/var/lib/postgresql/data -p 5432:5432 -d postgres\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ba3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "load_dotenv(\"rag_backend/.env\")\n",
    "\n",
    "\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "PG_DB   = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}?sslmode=disable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812a509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "#     saver.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b3b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph_builder = StateGraph(RAGState)\n",
    "\n",
    "    graph_builder.add_node(\"retrieve\", retrieve)\n",
    "    graph_builder.add_node(\"query_rag_llm\", query_rag_llm)\n",
    "    graph_builder.add_node(\"query_llm\", query_llm)\n",
    "\n",
    "    graph_builder.add_conditional_edges(\"retrieve\", route_rag_usage)\n",
    "    graph_builder.add_edge(\"query_rag_llm\", END)\n",
    "    graph_builder.add_edge(\"query_llm\", END)\n",
    "\n",
    "    graph_builder.set_entry_point(\"retrieve\")\n",
    "\n",
    "\n",
    "    graph = graph_builder.compile(checkpointer=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98c6ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'query': 'What is my name', 'docs': 'import java.awt.Color;\\n\\n/**\\n * Provide a counter for a participant in the simulation.\\n * This includes an identifying string and a count of how\\n * many participants of this type currently exist within \\n * the simulation.\\n *\\n * @version 2016.02.29\\n */\\npublic class Counter\\n{\\n    // A name for this type of simulation participant\\n    private String name;\\n    // How many of this type exist in the simulation.\\n    private int count;\\n\\n    /**\\n     * Provide a name for one of the simulation types.\\n     * @param name  A name, e.g. \"Fox\".\\n     */\\n    public Counter(String name)\\n    {\\n        this.name = name;\\n        count = 0;\\n    }\\n    \\n    /**\\n     * @return The short description of this type.\\n     */\\n    public String getName()\\n    {\\n        return name;\\n    }\\n\\n    /**\\n     * @return The current count for this type.\\n     */\\n    public int getCount()\\n    {\\n        return count;\\n    }\\n\\n    /**\\n     * Increment the current count by one.\\n     */\\n    public void increment()\\n    {\\n        count++;\\n    }\\n    \\n    /**\\n     * Reset the current count to zero.\\n     */\\n    public void reset()\\n    {\\n        count = 0;\\n    }\\n}\\n\\n\\nimport java.util.List;\\n\\n/**\\n * A class representing shared characteristics of animals.\\n *\\n * @version 2016.02.29 (2)\\n */\\npublic abstract class Animal\\n{\\n    // Whether the animal is alive or not.\\n    private boolean alive;\\n    // The animal\\'s field.\\n    private Field field;\\n    // The animal\\'s position in the field.\\n    private Location location;\\n    \\n    /**\\n     * Create a new animal at location in field.\\n     * \\n     * @param field The field currently occupied.\\n     * @param location The location within the field.\\n     */\\n    public Animal(Field field, Location location)\\n    {\\n        alive = true;\\n        this.field = field;\\n        setLocation(location);\\n    }\\n    \\n    /**\\n     * Make this animal act - that is: make it do\\n     * whatever it wants/needs to do.\\n     * @param newAnimals A list to receive newly born animals.\\n     */\\n    abstract public void act(List<Animal> newAnimals);\\n\\n    /**\\n     * Check whether the animal is alive or not.\\n     * @return true if the animal is still alive.\\n     */\\n    protected boolean isAlive()\\n    {\\n        return alive;\\n    }\\n\\n    /**\\n     * Indicate that the animal is no longer alive.\\n     * It is removed from the field.\\n     */\\n    protected void setDead()\\n    {\\n        alive = false;\\n        if(location != null) {\\n            field.clear(location);\\n            location = null;\\n            field = null;\\n        }\\n    }\\n\\n    /**\\n     * Return the animal\\'s location.\\n     * @return The animal\\'s location.\\n     */\\n    protected Location getLocation()\\n    {\\n        return location;\\n    }\\n    \\n    /**\\n     * Place the animal at the new location in the given field.\\n     * @param newLocation The animal\\'s new location.\\n     */\\n    protected void setLocation(Location newLocation)\\n    {\\n        if(location != null) {\\n            field.clear(location);\\n        }\\n        location = newLocation;\\n        field.place(this, newLocation);\\n    }\\n    \\n    /**\\n     * Return the animal\\'s field.\\n     * @return The animal\\'s field.\\n     */\\n    protected Field getField()\\n    {\\n        return field;\\n    }\\n}\\n\\n\\n5 Submission\\nThe submission consists of two parts: your code and a report documenting your submission. The\\ncode and report must be submitted before the deadline, by both members of your pair .\\n5.1 Code\\nYou have to submit a Jar of your project to the “Assignment 3: Code Submission” link in the\\nAssignment 3 section on the PPA KEATS page, before the due date. The Jar ﬁle must contain\\nyour source code, i.e., the *.java ﬁles, and runs on BlueJ .\\n5.2 Report\\nThe report must be submitted to the “Assignment 3: Report Submission” link in the Assignment\\n3 section on the PPA KEATS page, before the deadline. The report should be no more than four\\npages long.\\nThe report should contain the following :\\n• The names and student numbers of all students who worked on the submission.\\n• A description of your simulation, including the types of species that you are simulating, their\\nbehaviour and interactions.\\n• A list and description of all extension tasks you have implemented.\\n6 Deadline\\nAfter submitting your work on KEATS, check that (1) your Jar ﬁle and report (pdf) has been\\nsuccessfully uploaded and (2) that the Jar ﬁle contains all of your source ﬁles. This assignment\\n(code and report) is due Wednesday, March 2nd, 23:59.\\n7 Marking\\nApplications are marked based on four categories:\\n1. Program Correctness —The application meets all of the program speciﬁcations, i.e., the\\nstudent has completed all of the base tasks including following submission instructions (e.g.,\\nthe student submitted a Jar ﬁle of their BlueJ project).\\n2. Code Elegance —The application is written in such a way that the code is reusable and\\neﬃcient (i.e., memory usage and complexity). The application appropriately uses loops\\nand functions to reduce code complexity and/or repeated code. The application does not\\nhave hard-coded solutions or poorly designed solutions. A poorly designed solution is overly\\ncomplicated, utilises excessive amounts of memory or utilises a slower approach to a problem.\\n3. Documentation —The application is suﬃciently documented. Good documentation/comments\\nshould explain what the code does and how it does it. Comments can also be used to highlight\\nnuances in your solution, e.g., a segment of code that only works under certain conditions.\\n4. Readability —The application is easy to understand and uses good programming practices.\\nAdditional details on how marks are given can be found in the Marking Rubric.\\n3\\n', 'msg_state': {'thread_id': 1, 'messages': [SystemMessage(content=\"\\nBelow is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\\nYou are a helpful assistant that explains programming assignments.\\nYour task is to explain key terms, notions and user's questions. \\nDo not give any hints or direct solution of task even if you asked.\\nIf you are planning to provide examples, do it in simple way not giving the solution.\\nAnswer user's question in plain English and suggest how to approach it.\\nYou are enhanced AI model with previous prompt storage. Provide answers considering history\\nDo not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\\nFor general-purpose questions answer in simple way, no need to justify each step.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is alex', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm Alex. How can I help you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Alex! How may I assist you today?', additional_kwargs={}, response_metadata={})]}}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f05111d-b0de-6daf-8006-1fae8075c373'}}, metadata={'step': 6, 'source': 'loop', 'writes': {'query_rag_llm': {'msg_state': {'messages': [SystemMessage(content=\"\\nBelow is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\\nYou are a helpful assistant that explains programming assignments.\\nYour task is to explain key terms, notions and user's questions. \\nDo not give any hints or direct solution of task even if you asked.\\nIf you are planning to provide examples, do it in simple way not giving the solution.\\nAnswer user's question in plain English and suggest how to approach it.\\nYou are enhanced AI model with previous prompt storage. Provide answers considering history\\nDo not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\\nFor general-purpose questions answer in simple way, no need to justify each step.\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='My name is alex', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello! I'm Alex. How can I help you today?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is my name', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Alex! How may I assist you today?', additional_kwargs={}, response_metadata={})], 'thread_id': 1}}}, 'parents': {}, 'user_id': 1, 'thread_id': '1'}, created_at='2025-06-24T15:42:37.990223+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f05111d-a70f-6e76-8005-8f5b5be0dfad'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph = graph_builder.compile(checkpointer=saver)\n",
    "    print(graph.get_state(config={\"configurable\":{\"user_id\": 2, \"thread_id\":1}}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdaeeadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28282/2803283964.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(state.query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! I'm Alex. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph = graph_builder.compile(checkpointer=saver)\n",
    "\n",
    "    config={\"configurable\":{\"thread_id\":1, \"user_id\": 1}}\n",
    "    chat_history = MessagesState(\n",
    "        thread_id=1,\n",
    "        messages=[\n",
    "            SystemMessage(content=SYSTEM_PROMPT)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    input_message = \"My name is alex\"\n",
    "\n",
    "\n",
    "    input_state = RAGState(\n",
    "        query=input_message,\n",
    "        docs='',\n",
    "        msg_state=chat_history\n",
    "    )\n",
    "\n",
    "    response_state=graph.invoke(input_state, config=config)\n",
    "    for message in response_state['msg_state'][\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a2f1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! I'm Alex. How can I help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex! How may I assist you today?\n"
     ]
    }
   ],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph = graph_builder.compile(checkpointer=saver)\n",
    "\n",
    "    config={\"configurable\":{\"thread_id\":1, \"user_id\": 1}}\n",
    "\n",
    "\n",
    "\n",
    "    input_message = \"What is my name\"\n",
    "\n",
    "\n",
    "    input_state = RAGState(\n",
    "        query=input_message,\n",
    "        docs='',\n",
    "        msg_state=graph.get_state(config=config).values[\"msg_state\"]\n",
    "    )\n",
    "\n",
    "    response_state=graph.invoke(input_state, config=config)\n",
    "    for message in response_state['msg_state'][\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34c24780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Response: {'assignment_id': '123', 'content': 'Your name is Mike.'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8081/ask\"\n",
    "\n",
    "payload = {\n",
    "    \"assignment_id\": \"123\",\n",
    "    \"uuid\": \"user-abc\",\n",
    "    \"content\": \"What is my name\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(\"Status code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
