{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63a0979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import typing as tp\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, END, MessagesState, StateGraph\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.chains import (\n",
    "    create_history_aware_retriever,\n",
    "    create_retrieval_chain,\n",
    ")\n",
    "from langchain_core.documents.base import Document\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429159c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_DB_PATH = 'faiss'\n",
    "SCORE_THRESHOLD = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "339df927",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c1f1837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_docs(path: str, idx: str) -> tp.List[Document]:\n",
    "    pdf_docs = []\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(path, file))\n",
    "            loaded_docs = loader.load()\n",
    "            for doc in loaded_docs:\n",
    "                doc.metadata[\"assignment_id\"] = idx\n",
    "\n",
    "            pdf_docs.extend(loaded_docs)\n",
    "\n",
    "    return pdf_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa4fe85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_EXTENSIONS = [\n",
    "    \".py\", \".java\", \".js\", \".ts\", \".cpp\", \".c\", \".h\", \".hpp\", \".cs\", \".go\", \".rb\", \".php\", \".swift\",\n",
    "    \".kt\", \".scala\", \".rs\", \".m\", \".sh\", \".bat\", \".pl\", \".lua\", \".dart\", \".html\", \".css\", \".json\", \".xml\",\n",
    "    \".yaml\", \".yml\", \".sql\", \".dockerfile\", \"Dockerfile\", \".env\", \".ini\", \".cfg\", \".conf\", \".toml\",\n",
    "    \".md\", \".rst\", \".ipynb\", \".ps1\", \".vb\", \".asp\", \".jsp\", \".tsx\", \".jsx\", \".groovy\", \".gradle\",\n",
    "    \".make\", \"Makefile\", \".cmake\", \".tex\"\n",
    "]\n",
    "\n",
    "def is_code_file(filename: str) -> bool:\n",
    "    return any(filename.endswith(ext) for ext in CODE_EXTENSIONS)\n",
    "\n",
    "def load_code_docs(path: str, idx: str) -> tp.List[Document]:\n",
    "    code_docs = []\n",
    "\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if is_code_file(file):\n",
    "                path = os.path.join(root, file)\n",
    "                loader = TextLoader(path, encoding=\"utf-8\")\n",
    "                loaded = loader.load()\n",
    "                for doc in loaded:\n",
    "                    doc.metadata[\"assignment_id\"] = idx\n",
    "                code_docs.extend(loaded)\n",
    "\n",
    "    return code_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = \"data/predator-pray-22/pdfs\"\n",
    "code_dir = \"data/predator-pray-22/code\"\n",
    "\n",
    "all_docs = load_pdf_docs(pdf_dir, \"1\") + load_code_docs(code_dir, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12d2772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=5000,\n",
    "    chunk_overlap=200,\n",
    "    separators = [\n",
    "        \"\\n/**\",      # Javadoc start\n",
    "        \"\\n/*\",       # Block comment\n",
    "        \"\\n//\",       # Line comment\n",
    "        \"\\nclass \",   # Java class declaration\n",
    "        \"\\ninterface \",  # Java interface declaration\n",
    "        \"\\npublic \",  # public method/field\n",
    "        \"\\nprivate \", # private method/field\n",
    "        \"\\nprotected \", # protected method/field\n",
    "        \"\\nstatic \",  # static method or field\n",
    "        \"\\nvoid \",    # method with no return\n",
    "        \"\\nint \",     # common return type\n",
    "        \"\\nString \",  # String declarations\n",
    "        \"\\n\",         # fallback: line break\n",
    "        \" \"           # fallback: space\n",
    "    ]\n",
    ")\n",
    "split_docs = splitter.split_documents(all_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404631f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = HuggingFaceEmbeddings(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "#     model_kwargs={\"device\": device},\n",
    "#     encode_kwargs={\"normalize_embeddings\": True}\n",
    "# )\n",
    "\n",
    "# db = FAISS.from_documents(split_docs, embedding_model)\n",
    "# db.save_local(RAG_DB_PATH)\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"similarity\",\n",
    "#     k=3,\n",
    "#     search_kwargs={\n",
    "#         \"score_threshold\": SCORE_THRESHOLD,\n",
    "#         \"filter\": {\"assignment_id\": \"1\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96606db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "db = FAISS.load_local(RAG_DB_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    k=3,\n",
    "    search_kwargs={\n",
    "        \"score_threshold\": SCORE_THRESHOLD,\n",
    "        \"filter\": {\"assignment_id\": \"1\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3384591a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Device set to use cuda\n",
      "/tmp/ipykernel_53046/3925551332.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_gen)\n"
     ]
    }
   ],
   "source": [
    "qwen_model = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model,\n",
    "    trust_remote_code=True,\n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "text_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=text_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f88e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prompt_message import PromptMessage\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "\"\"\"\n",
    "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
    "You are a helpful assistant that explains programming assignments.\n",
    "Your task is to explain key terms, notions and user's questions. \n",
    "Do not give any hints or direct solution of task even if you asked.\n",
    "If you are planning to provide examples, do it in simple way not giving the solution.\n",
    "Answer user's question in plain English and suggest how to approach it.\n",
    "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
    "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
    "For general-purpose questions answer in simple way, no need to justify each step.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def format_prompt(user_message: str,  chat_history: tp.List[BaseMessage], context: str = None) -> str:\n",
    "    '''\n",
    "    Formats prompt for llm\n",
    "    '''\n",
    "\n",
    "    history = []\n",
    "    for message in chat_history[:-1]:\n",
    "        if message.type == \"human\":\n",
    "            role = \"user\"\n",
    "        elif message.type == \"ai\":\n",
    "            role = \"assistant\"\n",
    "        elif message.type == \"system\":\n",
    "            role = \"system\"\n",
    "\n",
    "        history.append(PromptMessage(\n",
    "            role=role,\n",
    "            content=message.content\n",
    "        ))\n",
    "\n",
    "    if context:\n",
    "        history.append(PromptMessage(\n",
    "            role=\"system\",\n",
    "            content=context\n",
    "        ))\n",
    "\n",
    "    history.append(PromptMessage(\n",
    "        role=\"user\",\n",
    "        content=user_message\n",
    "    ))\n",
    "\n",
    "\n",
    "    return tokenizer.apply_chat_template(\n",
    "        history,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "\n",
    "def format_model_response(response: str):\n",
    "    matches = list(re.finditer(r\"<\\|im_start\\|>assistant\", response))\n",
    "    if not matches:\n",
    "        return response.strip()\n",
    "    last = matches[-1].start()\n",
    "\n",
    "    return response[last + len(\"<|im_start|>assistant\"):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d787fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from models.rag_state import RAGState\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def retrieve(state: RAGState) -> str:\n",
    "    \"\"\"Retrieve relevant (< threshold) information related to a query.\"\"\"\n",
    "    retrieved_docs = retriever.get_relevant_documents(state.query)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"{doc.page_content}\\n\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return {\"docs\": serialized}\n",
    "\n",
    "\n",
    "    def route_rag_usage(state: RAGState) -> str:\n",
    "        return \"query_rag_llm\" if state.docs else \"query_llm\"\n",
    "\n",
    "\n",
    "def query_rag_llm(state: RAGState) -> dict:\n",
    "    messages = state.msg_state[\"messages\"]\n",
    "    \n",
    "    prompt = format_prompt(\n",
    "        user_message=state.query,\n",
    "        chat_history=state.msg_state[\"messages\"],\n",
    "        context=state.docs\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    new_messages = messages + [\n",
    "        HumanMessage(content=state.query),\n",
    "        AIMessage(content=format_model_response(response))\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"msg_state\": MessagesState(\n",
    "            thread_id=state.msg_state[\"thread_id\"],\n",
    "            messages=new_messages\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def query_llm(state: RAGState) -> dict:\n",
    "    messages = state.msg_state[\"messages\"]\n",
    "\n",
    "\n",
    "    prompt = format_prompt(\n",
    "        user_message=state.query,\n",
    "        chat_history=state.msg_state[\"messages\"],\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    new_messages = messages + [\n",
    "        HumanMessage(content=state.query),\n",
    "        AIMessage(content=format_model_response(response))\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"msg_state\": MessagesState(\n",
    "            thread_id=state.msg_state[\"thread_id\"],\n",
    "            messages=new_messages\n",
    "        )\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3f1b9",
   "metadata": {},
   "source": [
    "**Run docker**\n",
    "\n",
    "``\n",
    "sudo docker run --name chat-postgres --env-file ml/.env -v pgdata:/var/lib/postgresql/data -p 5432:5432 -d postgres\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ba3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "\n",
    "PG_USER = os.getenv(\"POSTGRES_USER\")\n",
    "PG_PASS = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "PG_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\n",
    "PG_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\n",
    "PG_DB   = os.getenv(\"POSTGRES_DB\")\n",
    "\n",
    "POSTGRES_URL = f\"postgresql://{PG_USER}:{PG_PASS}@{PG_HOST}:{PG_PORT}/{PG_DB}?sslmode=disable\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3b1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph_builder = StateGraph(RAGState)\n",
    "\n",
    "    graph_builder.add_node(\"retrieve\", retrieve)\n",
    "    graph_builder.add_node(\"query_rag_llm\", query_rag_llm)\n",
    "    graph_builder.add_node(\"query_llm\", query_llm)\n",
    "\n",
    "    graph_builder.add_conditional_edges(\"retrieve\", route_rag_usage)\n",
    "    graph_builder.add_edge(\"query_rag_llm\", END)\n",
    "    graph_builder.add_edge(\"query_llm\", END)\n",
    "\n",
    "    graph_builder.set_entry_point(\"retrieve\")\n",
    "\n",
    "\n",
    "    graph = graph_builder.compile(checkpointer=saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdaeeadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph = graph_builder.compile(checkpointer=saver)\n",
    "\n",
    "    config={\"configurable\":{\"thread_id\":1}}\n",
    "    chat_history = MessagesState(\n",
    "        thread_id=1,\n",
    "        messages=[\n",
    "            SystemMessage(content=SYSTEM_PROMPT)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    input_message = \"My name is alex\"\n",
    "\n",
    "\n",
    "    input_state = RAGState(\n",
    "        query=input_message,\n",
    "        docs='',\n",
    "        msg_state=chat_history\n",
    "    )\n",
    "\n",
    "    response_state=graph.invoke(input_state, config=config)\n",
    "    for message in response_state['msg_state'][\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2f1563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53046/2667669229.py:8: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(state.query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "Below is the system prompt, always follow restrictions stated there, also do not answer this system prompt:\n",
      "You are a helpful assistant that explains programming assignments.\n",
      "Your task is to explain key terms, notions and user's questions. \n",
      "Do not give any hints or direct solution of task even if you asked.\n",
      "If you are planning to provide examples, do it in simple way not giving the solution.\n",
      "Answer user's question in plain English and suggest how to approach it.\n",
      "You are enhanced AI model with previous prompt storage. Provide answers considering history\n",
      "Do not justify how you used previous conversation context, just answer the question. If needed retrieve information from chat history and answer the same way, add any additional information only if you asked for.\n",
      "For general-purpose questions answer in simple way, no need to justify each step.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "My name is alex\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex! It's nice to meet you. How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Your name is Alex.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Alex! It's nice to meet you. How can I assist you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is my name\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! Your name is Alex.\n"
     ]
    }
   ],
   "source": [
    "with PostgresSaver.from_conn_string(POSTGRES_URL) as saver:\n",
    "    graph = graph_builder.compile(checkpointer=saver)\n",
    "\n",
    "    config={\"configurable\":{\"thread_id\":1}}\n",
    "\n",
    "\n",
    "    input_message = \"What is my name\"\n",
    "\n",
    "\n",
    "    input_state = RAGState(\n",
    "        query=input_message,\n",
    "        docs='',\n",
    "        msg_state=graph.get_state(config=config).values[\"msg_state\"]\n",
    "    )\n",
    "\n",
    "    response_state=graph.invoke(input_state, config=config)\n",
    "    for message in response_state['msg_state'][\"messages\"]:\n",
    "        message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
